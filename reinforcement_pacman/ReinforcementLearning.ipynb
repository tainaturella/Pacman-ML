{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Project 3 - Pacman\n",
    "\n",
    "# Members:\n",
    "RA 183374 - Helena Steck\n",
    "\n",
    "RA 187251 - TainÃ¡ Turella C. dos Santos\n",
    "\n",
    "## Introduction\n",
    "During Project 3 - Pacman the group was required to apply its knowledge around genetic algorithms and reinforcement learning to teach Mr. Pacman to defeat the ghosts while he feeds himself in a maze.\n",
    "This project was harder than the previous projects because we needed to comprehend how the codified game works and then implement the algorithms in a way that they could interact with each other without friction/issues.\n",
    "\n",
    "## Code & Difficulties\n",
    "We used the pacman implementation that is available at [Berkeley](https://inst.eecs.berkeley.edu/~cs188/sp19/assets/files/search.zip \"Search.zip\") and over this implementation we managed to train our model using reinforcement learning.\n",
    "We were not able to comprehend how to implement the theory learned about genetic learning, therefore you won't be able to see information about genetic training in this report and we won't be able to make comparisons between the 2 models.\n",
    "\n",
    "## Reinforcement Learning\n",
    "The definition of Reinforcement Learning available in [Wikipedia](https://en.wikipedia.org/wiki/Reinforcement_learning \"Reinforcement learning\") is:\n",
    "> *Reinforcement learning (RL) is an area of machine learning concerned with how intelligent agents ought to take actions in an environment in order to maximize the notion of cumulative reward.Reinforcement learning is one of three basic machine learning paradigms, alongside supervised learning and unsupervised learning.\n",
    "Reinforcement learning differs from supervised learning in not needing labelled input/output pairs be presented, and in not needing sub-optimal actions to be explicitly corrected. Instead the focus is on finding a balance between exploration (of uncharted territory) and exploitation (of current knowledge).\n",
    "The environment is typically stated in the form of a Markov decision process (MDP), because many reinforcement learning algorithms for this context use dynamic programming techniques.The main difference between the classical dynamic programming methods and reinforcement learning algorithms is that the latter do not assume knowledge of an exact mathematical model of the MDP and they target large MDPs where exact methods become infeasible.*\n",
    "\n",
    "Our implementation will be based on those concepts using Approximate Q-Learning, which is a reinforcement learning algorithm that is based on knowing the value of an action given a particular state. The algorithim calculates the quality of a state-action combination and after each iteration the weigth of each feature is updated and at the end of the training the converged weights are used as params for the Agents.\n",
    "Using the Aproximate Q-Learning we tried to fit reward function as a linear function that is obtained due to the combination of the features applied and analysed in the model.\n",
    "One thing that the group believe that should be discussed is the impact that biases can cause in reinforcement learning algorithms. For example, in this project was very tough to teach pacman to eat the ghosts when they are \"scared\" that happens because in other point it is crucial for pacman to run away from them when they can be harmfull. This is just a silly example, but at the same time made us question somethings during the development.\n",
    "There are some advantages on using Aproximate Q-Learning, but there are also some disadvantages, such as; the restriction that it applies to the accuracy of learned rewards and it also requires well-known features. It is a more generic algorithm implementation, but its also not perfect.\n",
    "\n",
    "### Small Grid Training\n",
    "### Medium Grid Training\n",
    "### Classic Grid Training\n",
    "\n",
    "## Disclaimer:\n",
    "This project was equally divided between the two group members. Therefore we confirm that the methods, the discussions and the report elaboration were created by both of us. We used the code available at [Berkeley](https://inst.eecs.berkeley.edu/~cs188/sp19/assets/files/search.zip \"Search.zip\") to run pacman and we used their tutorial as a guide on how to implement the methods of Reinforcement Learning.\n",
    "\n",
    "We did the entire project during calls via Google Meets, using a pair programming method. In that way we could help each other during the development phase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by importing the necessary files from the pacman game and numpy for helping with math."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "%gui tk\n",
    "from game import Agent, Actions\n",
    "from pacman import runGames, readCommand\n",
    "from graphicsDisplay import PacmanGraphics\n",
    "from ghostAgents import RandomGhost\n",
    "import numpy as np\n",
    "import util\n",
    "import layout\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define a base class for our agent which abstracts all the game related things for us and also sets up states for training and running without training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseAgent(Agent):\n",
    "    def __init__(self, alpha=1.0, epsilon=0.05, gamma=0.8, numTraining=10):\n",
    "        \"\"\"\n",
    "        alpha    - learning rate\n",
    "        epsilon  - exploration rate\n",
    "        gamma    - discount factor\n",
    "        numTraining - number of training episodes\n",
    "        \"\"\"\n",
    "        self.alpha = float(alpha)\n",
    "        self.epsilon = float(epsilon)\n",
    "        self.gamma = float(gamma)\n",
    "        self.numTraining = int(numTraining)\n",
    "\n",
    "        self.episodesSoFar = 0\n",
    "        self.accumTrainRewards = 0.0\n",
    "        self.accumTestRewards = 0.0\n",
    "\n",
    "\n",
    "    def getLegalActions(self, state):\n",
    "        \"\"\"\n",
    "        Get the actions available for a given\n",
    "        state. This is what you should use to\n",
    "        obtain legal actions for a state\n",
    "        \"\"\"\n",
    "        return state.getLegalActions()\n",
    "\n",
    "\n",
    "    def observeTransition(self, state, action, nextState, deltaReward):\n",
    "        \"\"\"\n",
    "        Inform agent that a transition has\n",
    "        been observed. This will result in a call to self.update\n",
    "        on the same arguments\n",
    "        \"\"\"\n",
    "        self.episodeRewards += deltaReward\n",
    "        self.update(state, action, nextState, deltaReward)\n",
    "\n",
    "\n",
    "    def startEpisode(self):\n",
    "        \"\"\"\n",
    "        Called when new episode is starting\n",
    "        \"\"\"\n",
    "        self.lastState = None\n",
    "        self.lastAction = None\n",
    "        self.episodeRewards = 0.0\n",
    "\n",
    "\n",
    "    def stopEpisode(self):\n",
    "        \"\"\"\n",
    "        Called when episode is done\n",
    "        \"\"\"\n",
    "        if self.episodesSoFar < self.numTraining:\n",
    "            self.accumTrainRewards += self.episodeRewards\n",
    "        else:\n",
    "            self.accumTestRewards += self.episodeRewards\n",
    "        self.episodesSoFar += 1\n",
    "        if self.episodesSoFar >= self.numTraining:\n",
    "            # Take off the training wheels\n",
    "            self.epsilon = 0.0  # no exploration\n",
    "            self.alpha = 0.0  # no learning\n",
    "\n",
    "\n",
    "    def doAction(self, state, action):\n",
    "        \"\"\"\n",
    "        Called by inherited class when\n",
    "        an action is taken in a state\n",
    "        \"\"\"\n",
    "        self.lastState = state\n",
    "        self.lastAction = action\n",
    "\n",
    "        \n",
    "    def observationFunction(self, state):\n",
    "        \"\"\"\n",
    "        Called by Pacman game after a new state is generated\n",
    "        \"\"\"\n",
    "        if not self.lastState is None:\n",
    "            reward = state.getScore() - self.lastState.getScore()\n",
    "            self.observeTransition(\n",
    "                self.lastState, self.lastAction, state, reward\n",
    "            )\n",
    "        return state\n",
    "\n",
    "    \n",
    "    def registerInitialState(self, state):\n",
    "        \"\"\"\n",
    "        Called by Pacman game at the start of a game\n",
    "        \"\"\"\n",
    "        self.startEpisode()\n",
    "        if self.episodesSoFar == 0:\n",
    "            print(\"Beginning %d episodes of Training\" % (self.numTraining))\n",
    "\n",
    "\n",
    "    def final(self, state):\n",
    "        \"\"\"\n",
    "        Called by Pacman game at the terminal state\n",
    "        \"\"\"\n",
    "        deltaReward = state.getScore() - self.lastState.getScore()\n",
    "        self.observeTransition(\n",
    "            self.lastState, self.lastAction, state, deltaReward\n",
    "        )\n",
    "        self.stopEpisode()\n",
    "\n",
    "        # Make sure we have this var\n",
    "        if not \"episodeStartTime\" in self.__dict__:\n",
    "            self.episodeStartTime = time.time()\n",
    "        if not \"lastWindowAccumRewards\" in self.__dict__:\n",
    "            self.lastWindowAccumRewards = 0.0\n",
    "        self.lastWindowAccumRewards += state.getScore()\n",
    "\n",
    "        NUM_EPS_UPDATE = 100\n",
    "        if self.episodesSoFar % NUM_EPS_UPDATE == 0:\n",
    "            print(\"Reinforcement Learning Status:\")\n",
    "            windowAvg = self.lastWindowAccumRewards / float(NUM_EPS_UPDATE)\n",
    "            if self.episodesSoFar <= self.numTraining:\n",
    "                trainAvg = self.accumTrainRewards / float(self.episodesSoFar)\n",
    "                print(\n",
    "                    \"\\tCompleted %d out of %d training episodes\"\n",
    "                    % (self.episodesSoFar, self.numTraining)\n",
    "                )\n",
    "                print(\"\\tAverage Rewards over all training: %.2f\" % (trainAvg))\n",
    "            else:\n",
    "                testAvg = float(self.accumTestRewards) / (\n",
    "                    self.episodesSoFar - self.numTraining\n",
    "                )\n",
    "                print(\n",
    "                    \"\\tCompleted %d test episodes\"\n",
    "                    % (self.episodesSoFar - self.numTraining)\n",
    "                )\n",
    "                print(\"\\tAverage Rewards over testing: %.2f\" % testAvg)\n",
    "            print(\n",
    "                \"\\tAverage Rewards for last %d episodes: %.2f\"\n",
    "                % (NUM_EPS_UPDATE, windowAvg)\n",
    "            )\n",
    "            print(\n",
    "                \"\\tEpisode took %.2f seconds\"\n",
    "                % (time.time() - self.episodeStartTime)\n",
    "            )\n",
    "            self.lastWindowAccumRewards = 0.0\n",
    "            self.episodeStartTime = time.time()\n",
    "\n",
    "        if self.episodesSoFar == self.numTraining:\n",
    "            msg = \"Training Done (turning off epsilon and alpha)\"\n",
    "            print(\"%s\\n%s\" % (msg, \"-\" * len(msg)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we implement a function to help us filter the features of our pacman game state. This is important to reduce the complexity in training our Agent by reducing the state space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closestFood(pos, food, walls):\n",
    "    \"\"\"\n",
    "    Calculate the distance to the food closest to our pacman by\n",
    "    also taking in account the walls.\n",
    "\n",
    "    We do this by doing a fringe search on the 'graph'\n",
    "    represented by our game board and then for each position\n",
    "    we check if it has a food, if it has we found our target and\n",
    "    return the distance.\n",
    "    \"\"\"\n",
    "    fringe = [(pos[0], pos[1], 0)]\n",
    "    expanded = set()\n",
    "    while fringe:\n",
    "        # pop the first pos from the fringe\n",
    "        pos_x, pos_y, dist = fringe.pop(0)\n",
    "\n",
    "        # we already visited it: continue\n",
    "        if (pos_x, pos_y) in expanded:\n",
    "            continue\n",
    "\n",
    "        # else: add it to visited locations\n",
    "        expanded.add((pos_x, pos_y))\n",
    "\n",
    "        # if we find a food at this location then return the current distance\n",
    "        if food[pos_x][pos_y]:\n",
    "            return dist\n",
    "\n",
    "        # otherwise spread out from the location to its neighbours\n",
    "        nbrs = Actions.getLegalNeighbors((pos_x, pos_y), walls)\n",
    "        for nbr_x, nbr_y in nbrs:\n",
    "            fringe.append((nbr_x, nbr_y, dist + 1))\n",
    "\n",
    "    # no food found(probably we won the game? if not this will probably bug our Pacman :D)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFeatures(state, action):\n",
    "    \"\"\"\n",
    "    Returns the following features:\n",
    "    - bias(always one to ensure training will converge)\n",
    "    - whether food will be eaten\n",
    "    - how far away the next food is\n",
    "    - whether a ghost is one step away\n",
    "    \"\"\"\n",
    "    # extract the grid of food and wall locations and get the ghost locations\n",
    "    food = state.getFood()\n",
    "    walls = state.getWalls()\n",
    "    ghosts = state.getGhostPositions()\n",
    "\n",
    "    features = util.Counter()\n",
    "\n",
    "    features[\"bias\"] = 1.0\n",
    "\n",
    "    # compute the location of pacman after he takes the action\n",
    "    if action is not None:\n",
    "        x, y = state.getPacmanPosition()\n",
    "        dx, dy = Actions.directionToVector(action)\n",
    "        next_x, next_y = int(x + dx), int(y + dy)\n",
    "    else:\n",
    "        x, y = state.getPacmanPosition()\n",
    "        next_x, next_y = x, y\n",
    "\n",
    "    # count the number of ghosts 1-step away\n",
    "    features[\"#-of-ghosts-1-step-away\"] = sum(\n",
    "        (next_x, next_y) in Actions.getLegalNeighbors(g, walls)\n",
    "        for g in ghosts\n",
    "    )\n",
    "\n",
    "    # if there is no danger of ghosts then add the food feature\n",
    "    if not features[\"#-of-ghosts-1-step-away\"] and food[next_x][next_y]:\n",
    "        features[\"eats-food\"] = 1.0\n",
    "\n",
    "    dist = closestFood((next_x, next_y), food, walls)\n",
    "    if dist is not None:\n",
    "        # make the distance a number less than one otherwise the update\n",
    "        # will diverge wildly\n",
    "        features[\"closest-food\"] = float(dist) / (\n",
    "            walls.width * walls.height\n",
    "        )\n",
    "    features.divideAll(10.0)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have selected which features we want to have in our state, we can implement a class that trains our Agent using an Approximate Q-Learning strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ApproximateQAgent(BaseAgent):\n",
    "    \"\"\"\n",
    "    ApproximateQLearningAgent\n",
    "\n",
    "    You should only have to overwrite getQValue\n",
    "    and update.  All other QLearningAgent functions\n",
    "    should work as is.\n",
    "    \"\"\"\n",
    "    def __init__(self, **args):\n",
    "        BaseAgent.__init__(self, **args)\n",
    "        self.weights = util.Counter()\n",
    "\n",
    "\n",
    "    def getAction(self, state):\n",
    "        \"\"\"\n",
    "        Compute the action to take in the current state.  With\n",
    "        probability self.epsilon, we should take a random action and\n",
    "        take the best policy action otherwise.  Note that if there are\n",
    "        no legal actions, which is the case at the terminal state, you\n",
    "        should choose None as the action\n",
    "        \"\"\"\n",
    "        # get legal actions \n",
    "        action = None\n",
    "        legalActions = self.getLegalActions(state)\n",
    "        \n",
    "        # no action: return\n",
    "        if len(legalActions) == 0:\n",
    "            self.doAction(state, action)\n",
    "            return action\n",
    "\n",
    "        # explore new action or use Q-Value?\n",
    "        explore = util.flipCoin(self.epsilon)\n",
    "        \n",
    "        # not exploring: choose action with highest Q-Value\n",
    "        if not explore:\n",
    "            totalRewards = [self.getQValue(state, a) for a in legalActions]\n",
    "            action = legalActions[np.argmax(totalRewards)]\n",
    "        \n",
    "        # exploring: choose a random action\n",
    "        else:\n",
    "            action = random.choice(legalActions)\n",
    "\n",
    "        # inform base class of action picked\n",
    "        self.doAction(state, action)\n",
    "        \n",
    "        # return picked action\n",
    "        return action\n",
    "\n",
    "\n",
    "    def getQValue(self, state, action):\n",
    "        \"\"\"\n",
    "        Should return Q(state,action) = w * featureVector\n",
    "        where * is the dotProduct operator\n",
    "        \"\"\"\n",
    "        features = getFeatures(state, action)\n",
    "        return self.weights * features\n",
    "\n",
    "\n",
    "    def update(self, state, action, nextState, reward):\n",
    "        \"\"\"\n",
    "        Should update weights based on transition\n",
    "        \"\"\"\n",
    "        # get maximum Q-Value after this state change\n",
    "        legalNextActions = self.getLegalActions(nextState)\n",
    "        if len(legalNextActions) > 0:\n",
    "            max_q_value = np.max(\n",
    "                [self.getQValue(nextState, a) for a in legalNextActions]\n",
    "            )\n",
    "        else:\n",
    "            max_q_value = 0\n",
    "\n",
    "        # apply the Bellman equation to update the weights\n",
    "        difference = (reward + self.gamma * max_q_value) - self.getQValue(state, action)\n",
    "\n",
    "        incremented_features = getFeatures(state, action)\n",
    "        for k in incremented_features:\n",
    "            incremented_features[k] *= self.alpha * difference\n",
    "\n",
    "        self.weights += incremented_features\n",
    "\n",
    "\n",
    "    def final(self, state):\n",
    "        \"Called at the end of each game.\"\n",
    "        # call the super-class final method\n",
    "        BaseAgent.final(self, state)\n",
    "\n",
    "        # training finished: print current weights\n",
    "        if self.episodesSoFar == self.numTraining:\n",
    "            print(self.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning 1000 episodes of Training\n",
      "Reinforcement Learning Status:\n",
      "\tCompleted 100 out of 1000 training episodes\n",
      "\tAverage Rewards over all training: 169.60\n",
      "\tAverage Rewards for last 100 episodes: 169.60\n",
      "\tEpisode took 0.94 seconds\n",
      "Reinforcement Learning Status:\n",
      "\tCompleted 200 out of 1000 training episodes\n",
      "\tAverage Rewards over all training: 134.54\n",
      "\tAverage Rewards for last 100 episodes: 99.48\n",
      "\tEpisode took 0.85 seconds\n",
      "Reinforcement Learning Status:\n",
      "\tCompleted 300 out of 1000 training episodes\n",
      "\tAverage Rewards over all training: 166.28\n",
      "\tAverage Rewards for last 100 episodes: 229.76\n",
      "\tEpisode took 0.94 seconds\n",
      "Reinforcement Learning Status:\n",
      "\tCompleted 400 out of 1000 training episodes\n",
      "\tAverage Rewards over all training: 169.74\n",
      "\tAverage Rewards for last 100 episodes: 180.13\n",
      "\tEpisode took 0.92 seconds\n",
      "Reinforcement Learning Status:\n",
      "\tCompleted 500 out of 1000 training episodes\n",
      "\tAverage Rewards over all training: 171.72\n",
      "\tAverage Rewards for last 100 episodes: 179.61\n",
      "\tEpisode took 0.95 seconds\n",
      "Reinforcement Learning Status:\n",
      "\tCompleted 600 out of 1000 training episodes\n",
      "\tAverage Rewards over all training: 179.75\n",
      "\tAverage Rewards for last 100 episodes: 219.91\n",
      "\tEpisode took 0.96 seconds\n",
      "Reinforcement Learning Status:\n",
      "\tCompleted 700 out of 1000 training episodes\n",
      "\tAverage Rewards over all training: 166.83\n",
      "\tAverage Rewards for last 100 episodes: 89.31\n",
      "\tEpisode took 0.91 seconds\n",
      "Reinforcement Learning Status:\n",
      "\tCompleted 800 out of 1000 training episodes\n",
      "\tAverage Rewards over all training: 154.51\n",
      "\tAverage Rewards for last 100 episodes: 68.29\n",
      "\tEpisode took 0.94 seconds\n",
      "Reinforcement Learning Status:\n",
      "\tCompleted 900 out of 1000 training episodes\n",
      "\tAverage Rewards over all training: 151.60\n",
      "\tAverage Rewards for last 100 episodes: 128.29\n",
      "\tEpisode took 0.98 seconds\n",
      "Reinforcement Learning Status:\n",
      "\tCompleted 1000 out of 1000 training episodes\n",
      "\tAverage Rewards over all training: 152.32\n",
      "\tAverage Rewards for last 100 episodes: 158.81\n",
      "\tEpisode took 0.97 seconds\n",
      "Training Done (turning off epsilon and alpha)\n",
      "---------------------------------------------\n",
      "{'bias': 765.9329943564389, '#-of-ghosts-1-step-away': -5127.012576449337, 'closest-food': -1307.1033764973395, 'eats-food': 1355.4152662228264}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_TRAINING = 1000\n",
    "runGames(layout.getLayout('smallGrid'),\n",
    "         ApproximateQAgent(numTraining=NUM_TRAINING),\n",
    "         [RandomGhost(1), RandomGhost(2), RandomGhost(3), RandomGhost(4)],\n",
    "         PacmanGraphics(1.0, frameTime = 0.1),\n",
    "         numGames=NUM_TRAINING,\n",
    "         numTraining=NUM_TRAINING,\n",
    "         record=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
